# Go API Server Banner-service

## Конфигурация

- golang: 1.22
- postgres: 16
- VsCode

## Overview
This server was generated by the [openapi-generator]
(https://openapi-generator.tech) project.
By using the [OpenAPI-Spec](https://github.com/OAI/OpenAPI-Specification) from a remote server.

- API version: 1.0.0
- Build date: 2024-04-05T18:13:55.850283124+03:00[Europe/Moscow]
- Generator version: 7.4.0


### Запуск сервера
Для работы с сервером предоставлен Makefile со следующими целями: 
```
run_service // Локальный запуск сервера
```
```
staticcheck // Для статического анализа кода всего проекта, используя стандартную конфигурацию


checks = ["all", "-ST1000", "-ST1003", "-ST1016", "-ST1020", "-ST1021", "-ST1022", "-ST1023"]
initialisms = ["ACL", "API", "ASCII", "CPU", "CSS", "DNS", "EOF", "GUID", "HTML", "HTTP", "HTTPS", "ID", "IP", "JSON", "QPS", "RAM", "RPC", "SLA", "SMTP", "SQL", "SSH", "TCP", "TLS", "TTL", "UDP", "UI", "GID", "UID", "UUID", "URI", "URL", "UTF8", "VM", "XML", "XMPP", "XSRF", "XSS", "SIP", "RTP", "AMQP", "DB", "TS"]
dot_import_whitelist = ["github.com/mmcloughlin/avo/build", "github.com/mmcloughlin/avo/operand", "github.com/mmcloughlin/avo/reg"]
http_status_code_whitelist = ["200", "400", "404", "500"]

```

```
test // Запуск тестов
Тесты проводятся в пустую базу данных, так как все операции используют id баннера, в ином случае можно получить неожиданные результаты
```

```
stress_test // Замер времени выполнения опеций вставки и удаления на 1000RPS и 2000RPS
```

```
gen // Генерация данных в таблицу, отправкой 1000 пост запросов на сервер
```

```
compose-build-run // билд и запуск сервера в docker-compose
```

```
compose-build // билд сервера в docker-compose
```

```
compose-run // запуск сервера в docker-compose
```

```
compose-down // остановка docker-compose
```

### Сервер

В качестве маршрутизатора и диспетчера запросов используется пакет 
```
github.com/gorilla/mux
```
Для работы с базами данных используются пакеты
```
gorm.io/driver/postgres
gorm.io/gorm
```
Для логгирования используется пакет
```
log/slog
```
Используется cache in memory

Работа сервиса начинается с загрузки параметров окружения из файлов 
```
/env/dev/.env или /env/local/.dev. 
```
Один предназначен для запуска локально, а другой для запуска в контейнере
Далее создается объект кеша и базы данных. Сервис использует интерфейсы, так что можно менять реализации кеша так и используюмую базу данных.
Далее регистрируются все эндпоинты. Реализовано два middleware для логгирования и авторизации.
Каждый входящий запрос сначала проходит через middleware логгирования, затем через авторизацию.
Таким образом каждый запрос логгируется и записывается в файл:
```
/logs/log.txt
```
Записывается время когда был подан запрос, по какому пути, метод запроса, код возврата запроса и время выполнения запроса.
Пример записи логга для запроса:
```
time=2024-04-12T23:30:21.458+03:00 level=INFO msg="Request status" Path=/banner/9340 Method=DELETE status_code=204 elapsed_time=4.652321ms
```
middleware авторизации проверяет наличия токена в хедере и решает доступна ли операция для пользователя. Только потом идет логика выполнения запроса.

### Скрипт wait_postgres

Скрипт `wait_postgres.sh` Предназначен для того, чтобы сервер ждал пока postgres в контейнере не будет готов к работе. Он пытается подключиться к бд, пока не дождется ответа, как только от бд придет положительный ответ, скрипт сразу запускает сервис. А сервис в свою очередь уже успешно подключается к рабочей бд.

### База данных

По молчанию все DML конструкции в gorm выполняются в отдельной транзакции кроме SELECT и UPDATE, там транзакции нужно указывать явно

Связи между таблицами. Так как баннер может иметь много тегов встал вопрос как хранить данные. Было решено реализовать четыре таблицы. PK - primary key
1. Таблица `banners` с баннерами(PK ID)(хранит всю информацию по баннерам) кроме тегов и фич
2. Таблица `features` с фичами (PK ID)(вдруг фичи в будущем будут обозначаться еще чем-то кроме ID)
3. Таблица `tags` с тегами (PK ID)(вдруг теги в будущем будут обозначаться еще чем-то кроме ID)
4. Таблица `banner_tags` связывающая баннеры, теги и фичи - состоит из 3 аттрибутов, которые являются внешними ключами на три другие таблицы, теги и фичи образуют составной первичный ключ, который гарантирует уникальность

При запуске сервера gorm проверяют существуют ли необходимые таблицы в базе данных. Если нет то происходит автомиграция таблиц со всеми необходимыми связями. При автомиграции в таблице tags и features создаются по 5000 тысяч записей (Кол-во записей можно изменить в `.env` файлах). Если при вставке или измении баннеров будут указаны теги или фичи которых не существуют в тех двух таблицах, то запрос завершится ошибкой из-за ограничений внешних ключей таблицы `banner_tags`(Я думаю это верно, так как не нужно создавать банеры ссылающиеся на несуществующие теги или фичи). При вставке, изменении и удалении баннеров написаны хуки(Как триггеры, только хуки программно выполняются из го, а триггеры в бд). Хуки позволяют выполнят действия в рамках одной транзакции. Хуки проводят необходимые действия в таблице `banner_tags`


# Проблемы, с которыми столкнулся и их решения
- По условию задачи предположил, что токены админа и юзера - константы. Для этого реализовал еще один эндпоинт `/token` с методом `GET`. Токены генерируется пакетом `github.com/golang-jwt/jwt`. Он содержит внутри себя роль пользователя. Что и проверяется перед каждым запросом кроме запроса на получение токена соотвественно. Для простоты токены каждого пользователя одинаковые, как и токены админа
- По условию два банера с одним и теме же `feature_id` не могут иметь общий `tag_id`. Так как `/user_banner` должен отдавать только один баннер. Было принято решение связть первичным составным ключом `feature_id` и `tag_id`. Так же можно было создать уникальный составной индекс.
- Так же была проблемы с очень долгим временем обработки одновременных запросов к базе данных. Много запросов заканчивались ошибкой 500. Было принято увеличить количество открытых подключений к бд от 48 до 52. Числа выявленны эмпирически(процессор `AMD Ryzen 5 5600H` 6 ядер), при дальнейшем увеличении открытых подключений наблюдался только спад производительности. Что помогло выдержить 1000RPS(каждые 1мс отправлялся запрос) со средней вставкой 7мс и средним удалением в 6мс. При попытке выдержит 2000RPS(каждые 0.5мс отправляется запрос) первые 50-55 запросов выполняются с ожидаемым временем, а дальше происходит резкий провал по времени вплоть до 350мс. Я думаю это связано с тем, что по мере поступления запросов, все доступные подключения к базе данных заняты, новые запросы начинают ожидать освобождения подключения, что приводит к задержкам и увеличению времени обработки запросов
- При обновлении баннера, если новые данные по изменению тегов и/или фич нарушают ограничения уникальности тегов и фич банеров, то запрос вернется с ошибкой.
- Получение массивов из базы данных. Горм не поддерживает считывание массивов из базы данных в срезы на го. Но предоставляет возможность реализовать некоторые методы интерфейса для реализации считывания данных из базы данных в пользовательские типы на го. Было принято решение создать пользовательский тип на базе стандартного среза `[]int32`. Таким образом получилось сканировать `tag_id` в виде среза для конкретного баннера.
- Получение и передача JSON объектов на го в бд и обратно. gorm поддерживает передачу `[]byte` объектов, но толи из-за проблем с кодировкой то ли из-за чего-то еще, при передачи json объектов данные ломались и в бд записывалась не то что хотелось. Так же было принятно решение в реализации собственного типа танных `JSON` на базе `json.RawMessage` и реализации методов для правильного чтения и записи поля `content` у баннеров.
- Организация кеша. По условию: "при получении баннера передан флаг use_last_revision, необходимо отдавать самую актуальную информацию. В ином случае допускается передача информации, которая была актуальна 5 минут назад.". "При этом существует часть пользователей (порядка 10%), которым обязательно получать самую актуальную информацию. Для таких пользователей нужно предусмотреть механизм получения информации напрямую из БД." Думаю имелось в виду что при получении баннера не обязательно всегда заходить в бд. Кеш реализован in memory. И исходя по условию было принято решение что каждый попадающий в кеш баннер валиден 5 минут, а каждые 6 минут кеш самоочищается (Эти времена можно изменить в .env файлах). Так как тег и фича гарантируют уникальность банера, то ключом является его тег и фича, а хранится по ним только `content` баннера, так как получение баннера означает только получение его `content`.
При получении банера сначала проверяется кеш, если он получен из бд, то он записывается в кеш с временем жизни в 5 минут. Если из кеша, то данные просто отдаются без использования бд. Так же при Post методе созданный баннер заносится в бд, и так же записывается в кеш с первым поданым тегом при создании (Может запись в кеш при создании излишне).
- Написана программа для генерации 1000 баннеров в базе `tests/gen/generate_data.go`


# По доп. заданиям

1) Сервис адаптирован для увеличения количества фичей и тэгов, добавлял >10000, но и время ответа увеличится при запросах в базу.
2) Результаты тестов см. в [Tests.md](Tests.md)
3) Написаны 2e2 тесты для каждого эндпоинта в `/tests/end_to_end/` Предусмотрена цель в makefile. т.к. во время выполнения тестов выполняется цикл создания, обновления, получения, удаления и т.п. Перед запуском тестов таблицу в базе пересоздать. Иначе можно получить неожиданные результаты
4) linters: в makefile предмусмотрена цель staticcheck всего процета со стандартной конфигурацией:
- `checks = ["all", "-ST1000", "-ST1003", "-ST1016", "-ST1020", "-ST1021", "-ST1022", "-ST1023"]
initialisms = ["ACL", "API", "ASCII", "CPU", "CSS", "DNS", "EOF", "GUID", "HTML", "HTTP", "HTTPS", "ID", "IP", "JSON", "QPS", "RAM", "RPC", "SLA", "SMTP", "SQL", "SSH", "TCP", "TLS", "TTL", "UDP", "UI", "GID", "UID", "UUID", "URI", "URL", "UTF8", "VM", "XML", "XMPP", "XSRF", "XSS", "SIP", "RTP", "AMQP", "DB", "TS"]
dot_import_whitelist = ["github.com/mmcloughlin/avo/build", "github.com/mmcloughlin/avo/operand", "github.com/mmcloughlin/avo/reg"]
http_status_code_whitelist = ["200", "400", "404", "500"]`
- Так же в новых версиях расширения GO для VsCode встроен gopls. gopls испозует staticchect с расширенной конфигурацией Так же используется стандартная конфигурация с двумя параметрами:
`    "gopls": {
        "ui.completion.usePlaceholders": true,
        "staticcheck": true
    }`
